{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a929b03-c7c1-4cf2-8394-7380504e6962",
   "metadata": {},
   "source": [
    "## Saving, Loading, and Converting Models\n",
    "\n",
    "A challenge faced by bpnet-lite is that the official BPNet and ChromBPNet repositories are in TensorFlow. This means that optimal usage of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2946c3f-796e-4d6e-92b4-65f087a7c0ef",
   "metadata": {},
   "source": [
    "### Saving Models\n",
    "\n",
    "Let's start simple. Given a bpnet-lite model (either created using bpnet-lite or loaded into PyTorch using one of the techniques below) you can save the model in the same way you save any other PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb7def1-84ce-4b9e-8d1f-3acaeb4bfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from bpnetlite import BPNet\n",
    "\n",
    "toy_model = BPNet(n_filters=4, n_layers=2) # Make the model small to save disk space\n",
    "\n",
    "torch.save(toy_model, \"toy_bpnet_model.torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921eed67-ae50-433a-9b10-2bbfbe759c92",
   "metadata": {},
   "source": [
    "Because bpnet-lite attempts to be as low-level as possible, there are no additional wrappers or tricks for saving these models. Any feature that is present in PyTorch can be used out-of-the-box with bpnet-lite models.\n",
    "\n",
    "This works the same with ChromBPNet models. You can either save the entire model or save either of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7c5250-0051-4249-87c1-eba9881ff0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpnetlite import ChromBPNet\n",
    "\n",
    "accessibility = BPNet(n_filters=4, n_layers=2)\n",
    "bias = BPNet(n_filters=4, n_layers=2)\n",
    "\n",
    "toy_chrombpnet_model = ChromBPNet(bias, accessibility)\n",
    "\n",
    "torch.save(toy_chrombpnet_model, \"toy_chrombpnet_model.torch\")\n",
    "torch.save(toy_chrombpnet_model.bias, \"toy_chrombpnet_model.bias.torch\")\n",
    "torch.save(toy_chrombpnet_model.accessibility, \"toy_chrombpnet_model.accessibility.torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e4e04-08dd-4c72-8010-004f5bc6ae24",
   "metadata": {},
   "source": [
    "These models can also be saved after being wrapped, if you would like to do that. Personally, I do not do this because then I have to remember which models I have wrapped which ways. I would rather just save the base models and re-wrap them however I need to for each subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39ada66-458c-4473-942d-261593b95141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpnetlite.bpnet import CountWrapper\n",
    "\n",
    "toy_count_model = CountWrapper(toy_model)\n",
    "\n",
    "torch.save(toy_count_model, \"toy_count_model.torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a81cd-728f-45a7-bbb4-eaeb78570314",
   "metadata": {},
   "source": [
    "### Loading Models\n",
    "#### From PyTorch\n",
    "\n",
    "Loading models that have been trained using bpnet-lite is just as easy as loading any other PyTorch model. This can be done directly through the load command. If it's just the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60630f0e-8a7d-40be-8c4e-b3a19c025924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BPNet(\n",
       "  (iconv): Conv1d(4, 4, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "  (irelu): ReLU()\n",
       "  (rconvs): ModuleList(\n",
       "    (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (1): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "  )\n",
       "  (rrelus): ModuleList(\n",
       "    (0-1): 2 x ReLU()\n",
       "  )\n",
       "  (fconv): Conv1d(6, 2, kernel_size=(75,), stride=(1,), padding=(37,))\n",
       "  (linear): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_model2 = torch.load(\"toy_model.torch\", weights_only=False)\n",
    "toy_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78358a4b-d117-4569-9577-e253c6ce8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "If we have wrapped the models, we can also load them the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69403c9-781d-46e2-b8fb-8f49d47c2c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountWrapper(\n",
       "  (model): BPNet(\n",
       "    (iconv): Conv1d(4, 4, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "    (irelu): ReLU()\n",
       "    (rconvs): ModuleList(\n",
       "      (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      (1): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "    )\n",
       "    (rrelus): ModuleList(\n",
       "      (0-1): 2 x ReLU()\n",
       "    )\n",
       "    (fconv): Conv1d(6, 2, kernel_size=(75,), stride=(1,), padding=(37,))\n",
       "    (linear): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_count_model2 = torch.load(\"toy_count_model.torch\", weights_only=False)\n",
    "toy_count_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1461ca-e6c9-433e-b112-d6ac4ba3551f",
   "metadata": {},
   "source": [
    "#### From Official Repositories (ChromBPNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1f545-666b-4d18-86ef-8eaa1728c9ed",
   "metadata": {},
   "source": [
    "#### From tar.gz files\n",
    "\n",
    "On the ENCODE Portal, ChromBPNet models come in sets of five with one model trained on each of five cross-chromosomal folds. These models, along with important metadata, are packaged together and uploaded as a single tar.gz file. One could untar these files and then operate on the model files independently using the code above, but that might be inconvenient, result in too many files, or one may want to keep all of these compressed together. Conveniently, one can load models directly from these tar.gz files without needing to unpack them.\n",
    "\n",
    "To load directly from a tar.gz file, first you'll need to find where in the tar your model files are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc985cb2-196c-4e82-ada2-0e996e566f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "./fold_2\n",
      "./fold_2/model.chrombpnet_nobias.fold_2.ENCSR000EOT.tar\n",
      "./fold_2/model.bias_scaled.fold_2.ENCSR000EOT.h5\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.stdout_v1.txt\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.batch_loss.tsv\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.chrombpnet_model_params.tsv\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.args.json\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.epoch_loss.csv\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.chrombpnet_data_params.tsv\n",
      "./fold_2/logs.models.fold_2.ENCSR000EOT/logfile.modelling.fold_2.ENCSR000EOT.chrombpnet.params.json\n",
      "./fold_2/model.bias_scaled.fold_2.ENCSR000EOT.tar\n",
      "./fold_2/model.chrombpnet_nobias.fold_2.ENCSR000EOT.h5\n",
      "./fold_2/model.chrombpnet.fold_2.ENCSR000EOT.h5\n",
      "./fold_2/model.chrombpnet.fold_2.ENCSR000EOT.tar\n",
      "./fold_1\n",
      "./fold_1/model.bias_scaled.fold_1.ENCSR000EOT.tar\n",
      "./fold_1/model.chrombpnet.fold_1.ENCSR000EOT.tar\n",
      "./fold_1/model.chrombpnet_nobias.fold_1.ENCSR000EOT.tar\n",
      "./fold_1/model.chrombpnet.fold_1.ENCSR000EOT.h5\n",
      "./fold_1/model.chrombpnet_nobias.fold_1.ENCSR000EOT.h5\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.chrombpnet_model_params.tsv\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.args.json\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.batch_loss.tsv\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.stdout_v1.txt\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.epoch_loss.csv\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.chrombpnet_data_params.tsv\n",
      "./fold_1/logs.models.fold_1.ENCSR000EOT/logfile.modelling.fold_1.ENCSR000EOT.chrombpnet.params.json\n",
      "./fold_1/model.bias_scaled.fold_1.ENCSR000EOT.h5\n",
      "./README.md\n",
      "./logs.models.ENCSR000EOT\n",
      "./fold_3\n",
      "./fold_3/model.bias_scaled.fold_3.ENCSR000EOT.h5\n",
      "./fold_3/model.chrombpnet.fold_3.ENCSR000EOT.h5\n",
      "./fold_3/model.chrombpnet_nobias.fold_3.ENCSR000EOT.h5\n",
      "./fold_3/model.bias_scaled.fold_3.ENCSR000EOT.tar\n",
      "./fold_3/model.chrombpnet_nobias.fold_3.ENCSR000EOT.tar\n",
      "./fold_3/model.chrombpnet.fold_3.ENCSR000EOT.tar\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT/logfile.modelling.fold_3.ENCSR000EOT.epoch_loss.csv\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT/logfile.modelling.fold_3.ENCSR000EOT.chrombpnet_model_params.tsv\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT/logfile.modelling.fold_3.ENCSR000EOT.chrombpnet_data_params.tsv\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT/logfile.modelling.fold_3.ENCSR000EOT.stdout_v1.txt\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT/logfile.modelling.fold_3.ENCSR000EOT.chrombpnet.params.json\n",
      "./fold_3/logs.models.fold_3.ENCSR000EOT/logfile.modelling.fold_3.ENCSR000EOT.batch_loss.tsv\n",
      "./fold_0\n",
      "./fold_0/model.bias_scaled.fold_0.ENCSR000EOT.tar\n",
      "./fold_0/model.chrombpnet.fold_0.ENCSR000EOT.h5\n",
      "./fold_0/model.chrombpnet_nobias.fold_0.ENCSR000EOT.tar\n",
      "./fold_0/model.bias_scaled.fold_0.ENCSR000EOT.h5\n",
      "./fold_0/model.chrombpnet_nobias.fold_0.ENCSR000EOT.h5\n",
      "./fold_0/model.chrombpnet.fold_0.ENCSR000EOT.tar\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.stdout_v1.txt\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.chrombpnet_model_params.tsv\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.batch_loss.tsv\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.args.json\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.chrombpnet.params.json\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.epoch_loss.csv\n",
      "./fold_0/logs.models.fold_0.ENCSR000EOT/logfile.modelling.fold_0.ENCSR000EOT.chrombpnet_data_params.tsv\n",
      "./fold_4\n",
      "./fold_4/model.chrombpnet.fold_4.ENCSR000EOT.tar\n",
      "./fold_4/model.bias_scaled.fold_4.ENCSR000EOT.tar\n",
      "./fold_4/model.bias_scaled.fold_4.ENCSR000EOT.h5\n",
      "./fold_4/model.chrombpnet_nobias.fold_4.ENCSR000EOT.tar\n",
      "./fold_4/model.chrombpnet_nobias.fold_4.ENCSR000EOT.h5\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT/logfile.modelling.fold_4.ENCSR000EOT.chrombpnet_data_params.tsv\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT/logfile.modelling.fold_4.ENCSR000EOT.chrombpnet.params.json\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT/logfile.modelling.fold_4.ENCSR000EOT.chrombpnet_model_params.tsv\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT/logfile.modelling.fold_4.ENCSR000EOT.batch_loss.tsv\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT/logfile.modelling.fold_4.ENCSR000EOT.stdout_v1.txt\n",
      "./fold_4/logs.models.fold_4.ENCSR000EOT/logfile.modelling.fold_4.ENCSR000EOT.epoch_loss.csv\n",
      "./fold_4/model.chrombpnet.fold_4.ENCSR000EOT.h5\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"ENCFF574YLK.tar.gz\", \"r:gz\") as tar:\n",
    "    for filename in tar.getnames():\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c0e01-b4bd-4fe1-848d-d10fb64e59fc",
   "metadata": {},
   "source": [
    "We can see that the main structure of the tar.gz is five folders, with one folder for each of the five folds. Within each one there is an accessibility model and a bias model, logs and some other files. We want the .h5 file. We then need to decompress the accessibility and bias portions of the h5 (we do not need to read the entire thing into memory, which is nice), use the `BytesIO` wrapper to convert this stream into a fake readable file, and pass that into the existing code.\n",
    "\n",
    "Below is the entirety of the code to load up the ChromBPNet model directly from the tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea44803-ccda-400c-85e5-f6290faaffff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChromBPNet(\n",
       "  (bias): BPNet(\n",
       "    (iconv): Conv1d(4, 128, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "    (irelu): ReLU()\n",
       "    (rconvs): ModuleList(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "      (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "      (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "    )\n",
       "    (rrelus): ModuleList(\n",
       "      (0-3): 4 x ReLU()\n",
       "    )\n",
       "    (fconv): Conv1d(128, 1, kernel_size=(75,), stride=(1,), padding=(37,))\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (accessibility): BPNet(\n",
       "    (iconv): Conv1d(4, 512, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "    (irelu): ReLU()\n",
       "    (rconvs): ModuleList(\n",
       "      (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "      (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "      (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "      (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "      (6): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      (7): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(256,))\n",
       "    )\n",
       "    (rrelus): ModuleList(\n",
       "      (0-7): 8 x ReLU()\n",
       "    )\n",
       "    (fconv): Conv1d(512, 1, kernel_size=(75,), stride=(1,), padding=(37,))\n",
       "    (linear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (_log): _Log()\n",
       "  (_exp1): _Exp()\n",
       "  (_exp2): _Exp()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "from bpnetlite.chrombpnet import ChromBPNet\n",
    "\n",
    "with tarfile.open(\"ENCFF574YLK.tar.gz\", \"r:gz\") as tar:    \n",
    "    bias_tar = tar.extractfile(\"./fold_0/model.bias_scaled.fold_0.ENCSR000EOT.h5\").read()\n",
    "    accessibility_tar = tar.extractfile(\"./fold_0/model.chrombpnet_nobias.fold_0.ENCSR000EOT.h5\").read()\n",
    "\n",
    "chrombpnet = ChromBPNet.from_chrombpnet(\n",
    "    BytesIO(bias_tar),\n",
    "    BytesIO(accessibility_tar)\n",
    ")\n",
    "\n",
    "chrombpnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f2f27-c3a4-4516-9c8b-cf99ba7c4e2e",
   "metadata": {},
   "source": [
    "This results in exactly the same type of model as if one unzipped the file and loaded it individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88333d5c-62fb-41f8-9cb9-9bab2cfee353",
   "metadata": {},
   "source": [
    "### Converting Models\n",
    "\n",
    "At this point, converting models is simple. We know how to load models into PyTorch from a variety of formats, and we know how to save a model once it is in the bpnet-lite PyTorch format. Let's consider the situation where we want to download a model from the ENCODE Portal that was trained using the official TensorFlow repository and convert it into PyTorch. It's basically just one more line of code compared to the loading cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b8c9ea-d6c0-4cd8-a4db-83f01f40293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "from bpnetlite.chrombpnet import ChromBPNet\n",
    "\n",
    "with tarfile.open(\"ENCFF574YLK.tar.gz\", \"r:gz\") as tar:    \n",
    "    bias_tar = tar.extractfile(\"./fold_0/model.bias_scaled.fold_0.ENCSR000EOT.h5\").read()\n",
    "    accessibility_tar = tar.extractfile(\"./fold_0/model.chrombpnet_nobias.fold_0.ENCSR000EOT.h5\").read()\n",
    "\n",
    "chrombpnet = ChromBPNet.from_chrombpnet(\n",
    "    BytesIO(bias_tar),\n",
    "    BytesIO(accessibility_tar)\n",
    ")\n",
    "\n",
    "# New line of code here saving the model\n",
    "torch.save(chrombpnet, \"chrombpnet-test-model.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
